{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# (More) Nonlinear functions\n",
    "\n",
    "In the previous examples, we solved a constrained Rosenbrock problem and the sphere problem.\n",
    "\n",
    "While both of these problems are nonlinear, they're really just a mix of simple polynomials. This means that we can\n",
    "express it with overloaded Python operators: `+, -, *, /, **,` and so on.\n",
    "\n",
    "But what if we want to use a function that's not part of simple arithmetic - something like $\\cos(x)$, for example?\n",
    "That's what we'll explore here!\n",
    "\n",
    "-----\n",
    "\n",
    "One of the coolest things about the `Opti` stack is that it's fast - really, **really** fast. You can solve\n",
    "nonlinear, nonconvex optimization problems with thousands of variables in mere seconds on a laptop, thanks to\n",
    "automatic differentiation (AD) provided by CasADi and modern optimization methods via IPOPT.\n",
    "\n",
    "In order for AD to work, we need to be able to make a list (more precisely, a directed graph) of each mathematical\n",
    "operation (think `+, -, *, /, **, log(), fabs(),` etc.) that's applied throughout our optimization formulation (some\n",
    "call this list a \"trace\" in the literature).\n",
    "\n",
    "This means we can't just use NumPy out of the box like we'd like to,\n",
    "because some of its functions break our trace.\n",
    "\n",
    "Instead, we need to use a custom math library, which sounds scary at first. However, the AeroSandbox development team\n",
    "has tried to make this as seamless to you as possible - by writing our own NumPy with identical syntax! Here's how\n",
    "this works:\n",
    "\n",
    "* `aerosandbox.numpy` imports the entirety of NumPy.\n",
    "\n",
    "* For NumPy functions that break our AD trace (e.g. `np.sum()`), we've rewritten our own versions of them. This\n",
    "means:\n",
    "\n",
    "\t* If you pass normal NumPy arrays to these functions, they'll work 100% exactly the same as they would in\n",
    "\toriginal NumPy - same result and same speed.\n",
    "\n",
    "\t* If you pass optimization variables to these functions, they'll intelligently switch over to a version of the\n",
    "\tfunction that allows us to preserve the AD trace.\n",
    "\n",
    "* **So what does this mean for you, dear user?** It means that when working with AeroSandbox, all you need to do\n",
    "is replace `import numpy as np` with `import aerosandbox.numpy as np`, and you're good to go!\n",
    "\n",
    "* Caveat: Not all NumPy functions that should be overwritten have been overwritten - we've done our best,\n",
    "but there are *sooo* many obscure NumPy functions! If you get an error on a function you want to use,\n",
    "raise an issue ticket!\n",
    "\n",
    "You'll notice that in our last example, we imported `aerosandbox.numpy` in order to use the `sum()` function.\n",
    "\n",
    "Here, let's do an example with some other functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "[MX(fabs(opti0_lam_g_1)), MX(fabs(opti0_lam_g_2))]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import aerosandbox as asb\n",
    "import aerosandbox.numpy as np\n",
    "\n",
    "opti = asb.Opti()\n",
    "\n",
    "x = opti.variable(init_guess=3)\n",
    "\n",
    "f = np.exp(  # You can use normal operations from NumPy like this!\n",
    "    np.cos(  # These functions are intelligently overloading in the background...\n",
    "        x\n",
    "    )\n",
    ")\n",
    "\n",
    "opti.minimize(f)\n",
    "\n",
    "opti.subject_to([\n",
    "    x >= 0,\n",
    "    x <= np.pi / 2\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-26T03:17:16.237726500Z",
     "start_time": "2024-01-26T03:17:14.988130600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note another feature we just introduced here: you can give `opti.subject_to()` a list of constraints, not just a\n",
    "single constraint like we did before! Often, this makes for cleaner, more readable code.\n",
    "\n",
    "Also, note that you can declare variables, constraints, and objectives in any order. As long as they're all set in\n",
    "place by the time you call `sol = opti.solve()`, you're good. Speaking of, let's solve!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Ipopt version 3.14.11, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        2\n",
      "Number of nonzeros in Lagrangian Hessian.............:        1\n",
      "\n",
      "Total number of variables............................:        1\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        2\n",
      "        inequality constraints with only lower bounds:        1\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        1\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  3.7157948e-01 1.43e+00 2.48e-02   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  1.0109896e+00 0.00e+00 1.25e+00  -1.9 1.44e+00    -  1.00e+00 1.00e+00h  1\n",
      "   2  1.0081910e+00 0.00e+00 5.21e-04  -2.1 2.77e-03    -  9.99e-01 1.00e+00f  1\n",
      "   3  1.0001005e+00 0.00e+00 1.66e-04  -8.1 8.06e-03    -  9.87e-01 1.00e+00f  1\n",
      "   4  1.0000000e+00 0.00e+00 5.56e-08 -10.1 1.00e-04    -  1.00e+00 1.00e+00f  1\n",
      "   5  9.9999998e-01 0.00e+00 4.44e-16 -11.0 2.44e-08    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 5\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   9.9999998430203862e-01    9.9999998430203862e-01\n",
      "Dual infeasibility......:   4.4408920985006262e-16    4.4408920985006262e-16\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   1.0001999071900710e-11    1.0001999071900710e-11\n",
      "Overall NLP error.......:   1.0001999071900710e-11    1.0001999071900710e-11\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 6\n",
      "Number of objective gradient evaluations             = 6\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 6\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 6\n",
      "Number of Lagrangian Hessian evaluations             = 5\n",
      "Total seconds in IPOPT                               = 0.004\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n",
      "       nlp_f  |        0 (       0)   9.00us (  1.50us)         6\n",
      "       nlp_g  |        0 (       0)   7.00us (  1.17us)         6\n",
      "  nlp_grad_f  |        0 (       0)  14.00us (  2.00us)         7\n",
      "  nlp_hess_l  |        0 (       0)  11.00us (  2.20us)         5\n",
      "   nlp_jac_g  |        0 (       0)   4.00us (571.43ns)         7\n",
      "       total  |   4.00ms (  4.00ms)   3.88ms (  3.88ms)         1\n",
      "x = 1.5707963424928582\n"
     ]
    }
   ],
   "source": [
    "sol = opti.solve()\n",
    "\n",
    "x_opt = sol(x)\n",
    "\n",
    "print(f\"x = {x_opt}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-26T03:17:16.252353200Z",
     "start_time": "2024-01-26T03:17:16.235722200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice, it solved! The value of $x$ at the optimum turns out to be equal to $\\pi / 2 \\approx 1.57$.\n",
    "\n",
    "Note that there are tons and tons of nonlinear functions you can use - everything from logarithms to vector norms to\n",
    "linear solves to eigenvalue decompositions. The list is quite extensive and can be viewed at:\n",
    "`aerosandbox/numpy/test_numpy/test_all_operations_run.py`, where many of the valid operations are listed.\n",
    "\n",
    "This would not be possible without tons of hard work by the CasADi team!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
